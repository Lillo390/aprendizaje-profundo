{"cells":[{"cell_type":"markdown","metadata":{"id":"SQxgjupdXmjW"},"source":["# **Ejemplo de uso del framework TF Agents en Space Invaders**\n","\n","# ** NO FUNCIONA: 5/4/2023\n","mirar: https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb#scrollTo=0pTbJ3PeyF-u\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75284,"status":"ok","timestamp":1680681283416,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"},"user_tz":-120},"id":"9KQhYThvTCQC","outputId":"420daea3-d6a2-44ed-ebc4-14465af819e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Note: using Google CoLab\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-525\n","Use 'sudo apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n","Need to get 780 kB of archives.\n","After this operation, 2,271 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.8 [780 kB]\n","Fetched 780 kB in 1s (1,050 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package xvfb.\n","(Reading database ... 128293 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.20.13-1ubuntu1~20.04.8_amd64.deb ...\n","Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n","Setting up xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.3/831.3 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","scikit-image 0.19.3 requires imageio>=2.4.1, but you have imageio 2.4.0 which is incompatible.\n","moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Instalar librerías\n","try:\n","    from google.colab import drive\n","    %tensorflow_version 2.x\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False\n","\n","if COLAB:\n","  !sudo apt-get install -y xvfb ffmpeg\n","  !pip install -q 'gym==0.10.11'\n","  !pip install -q 'imageio==2.4.0'\n","  !pip install -q PILLOW\n","  !pip install -q 'pyglet==1.3.2'\n","  !pip install -q pyvirtualdisplay\n","  !pip install -q --upgrade tensorflow-probability\n","  !pip install -q tf-agents"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LizGqir9XC9U","executionInfo":{"status":"ok","timestamp":1680681373608,"user_tz":-120,"elapsed":4702,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}},"outputId":"0578d70b-4afa-4bac-f08b-b23e9de4fff1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ale-py\n","  Downloading ale_py-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from ale-py) (1.22.4)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py) (5.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from ale-py) (4.5.0)\n","Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.9/dist-packages (from ale-py) (6.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.10.0->ale-py) (3.15.0)\n","Installing collected packages: ale-py\n","Successfully installed ale-py-0.8.1\n"]}],"source":["!pip install ale-py"]},{"cell_type":"code","source":["# Roms de Atari ya que Gym no provee las Roms\n","# ! wget http://www.atarimania.com/roms/Roms.rar\n","# ! mkdir /content/ROM/\n","# ! unrar e /content/Roms.rar /content/ROM/\n","# ! python -m atari_py.import_roms /content/ROM/"],"metadata":{"id":"lOkGD-0caqex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install gym[atari,accept-rom-license]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdrcD8bjaq_K","executionInfo":{"status":"ok","timestamp":1680681419932,"user_tz":-120,"elapsed":28809,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}},"outputId":"e1eb0167-11a9-4aa5-d03e-ab117e983cf0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.9/dist-packages (0.23.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (6.1.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (1.22.4)\n","Collecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Collecting ale-py~=0.7.4\n","  Downloading ale_py-0.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py~=0.7.4->gym[accept-rom-license,atari]) (5.12.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.65.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.27.1)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.6.0.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.10.0->gym[accept-rom-license,atari]) (3.15.0)\n","Collecting libtorrent\n","  Downloading libtorrent-2.0.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.12.7)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.0-py3-none-any.whl size=446686 sha256=4e440cac26fc79b4f57eb304c560b313b400c20034ab6d27bd094b5fff8c6101\n","  Stored in directory: /root/.cache/pip/wheels/7d/17/c9/c31922a6aaf4ec7ec90eeee5dbc40ffbaafeda64b30a208b72\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: libtorrent, AutoROM.accept-rom-license, autorom, ale-py\n","  Attempting uninstall: ale-py\n","    Found existing installation: ale-py 0.8.1\n","    Uninstalling ale-py-0.8.1:\n","      Successfully uninstalled ale-py-0.8.1\n","Successfully installed AutoROM.accept-rom-license-0.6.0 ale-py-0.7.5 autorom-0.4.2 libtorrent-2.0.7\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"RYMQKVau-kFf","executionInfo":{"status":"ok","timestamp":1680681427829,"user_tz":-120,"elapsed":809,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["#### SI DA ERROR, REINICIAR ENTORNO Y SEGUIR EJECUTANDO ####"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"sMitx5qSgJk1","executionInfo":{"status":"ok","timestamp":1680681433952,"user_tz":-120,"elapsed":5761,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["import base64\n","import imageio\n","import IPython\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL.Image\n","import pyvirtualdisplay\n","\n","import tensorflow as tf\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.drivers import dynamic_step_driver\n","from tf_agents.environments import suite_gym, suite_atari, suite_atari_test\n","from tf_agents.environments import tf_py_environment, batched_py_environment\n","from tf_agents.eval import metric_utils\n","from tf_agents.metrics import tf_metrics\n","from tf_agents.networks import q_network, network\n","from tf_agents.policies import random_tf_policy \n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import trajectory\n","from tf_agents.utils import common\n","from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n","from tf_agents.networks import categorical_q_network\n","\n","from tf_agents.specs import tensor_spec\n","from tf_agents.trajectories import time_step as ts"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"EuPIJKcadAIB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680681438379,"user_tz":-120,"elapsed":253,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}},"outputId":"ef82e9e7-d8e6-478c-c1c7-15dd03924d4d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf_agents.environments.suite_atari_test.SuiteAtariTest testMethod=runTest>"]},"metadata":{},"execution_count":9}],"source":["suite_atari_test.SuiteAtariTest()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4vxg9BZLXcJC","executionInfo":{"status":"ok","timestamp":1680681440688,"user_tz":-120,"elapsed":633,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["# VirtualDisplay para los entornos de gym\n","display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"HC1kNrOsLSIZ","executionInfo":{"status":"ok","timestamp":1680681442331,"user_tz":-120,"elapsed":12,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["num_iterations = 50 #250000 \n","\n","initial_collect_steps = 200  \n","collect_steps_per_iteration = 10 \n","replay_buffer_max_length = 100000\n","\n","batch_size =   32\n","learning_rate = 2.5e-3\n","log_interval =   5000\n","\n","num_eval_episodes = 5  \n","eval_interval = 25000"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pYEz-S9gEv2-","executionInfo":{"status":"ok","timestamp":1680681450621,"user_tz":-120,"elapsed":281,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["#env_name = 'Breakout-v4'\n","#env_name = 'Pong-v0'\n","#env_name = 'BreakoutDeterministic-v4'\n","env_name = 'SpaceInvaders-v4'\n","\n","\n","# AtariPreprocessing runs 4 frames at a time, max-pooling over the last 2\n","# frames. We need to account for this when computing things like update\n","# intervals.\n","ATARI_FRAME_SKIP = 4\n","\n","max_episode_frames=108000  # ALE frames\n","\n","# Ejemplo de lectura de el entorno\n","env = suite_atari.load(\n","    env_name,\n","    max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,\n","    gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"RlO7WIQHu_7D","colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"status":"ok","timestamp":1680681453720,"user_tz":-120,"elapsed":545,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}},"outputId":"283e4fd8-d2e9-4b2d-b714-e9e67a2bec5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=160x210 at 0x7F7D7F4F9BB0>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAADn0lEQVR4nO3dsa7TVhgHcF/EM3QqDJ0qVV2YeIcuFUwsPEjUscqDsMPWd2BiqZA6dejlQRiCbq3cOD4mx/6Ov/P7DYhEf4xPPn9x7OM4wwAAAAAAe3A3fvDizxez/+DTH59WW5lza6/Pu+PL2czbw8fvXv5Sa6zP3Xxk2tICtLYBLbW0AC1sQJMdPFWYqA5eY33GBZgqTFQH11qfah1cUoDWNqCllhaghQ1IB3+jgy/QwXWfX4MO/kYHX6CD6z6/BsfBMxwHz3AcXJ4HAAAAAGCR4/GZfB5nw5sdbW/5fTsN72GQha9OP/ndGw+1ZLS95WPdNB88djbUw+FevgVPqizlNNrTIE9/Xt+0e8sHqtPBj4d3fYvuLR+oTgeTXGuHJa3lA1Xo4IuHDVfG3FseAAAA2Moqs0klJ2Z7y0epOZs09VA+ULXJhoetuHBz7i0fpcJb9NT2OzXy3vK719rJ/dbysZ7WWtDhcD++zkG+ESb8mdPaW2Jr+Vg6ODkFBgAAAPjfxbN0s6f6+snHciYruWoFPjv5Lt+IavPBw/Kh9pbfsYcps8K5s97ySRwX3nWmt3wIH7KSU2DKtPaptbX8jj3eFV3fOfWWBwAAALbiHh0b5aO4R8cW+UDVbic8FN/ivrd8LLf03yIfqM5b9NnwZkfbWz6D1qbnWstHMeFPgdY+tbaWD6SDmXNx+y05zOgkDwAAAGzFFR0b5aO4omOLfKCaV3QMw3A43JdfEdFJPpbJhuRq3oRlWL4h95bfq9M4T1+TLRlzb/ndGw+y/AXqJx/IPjg5BU6u5q0MxxcalhxKdpUHAAAA9mXp8X5v+RCu6NgiH6jyFR3Xn+wzH8vP6myU37fW3hJbywcyXUiB1q6gaC2/b629mq3lY3mLZk5rn1dbywMAAAAAAAAAAAAAAPSq2q+u3OKv1z8//P239/9YfkW+fJZcfIHHm//jh5Z/o/gCsyoFTk6Bk1Pg5BQ4OQVOToGTCy7wxaPGioeSe1/+7XRwcgqcnAInp8DJKXBylX8B/PuM51DX+Ai69+XfQgcDAAAAAA1o4rtJUd4dX85m3h4+brAm63EuOrmuO3jKuLN1ME1rYj44Ssk+eO90cHL2wRfYB7Mb9sHJ6eDkZvbBv7/5YZv1YCWTBa5Y2v9++XEYhuefv9Ra4H69+vWnYRg+/P3vZv/jUz2am31wcgqcnAInd2cfnJsOTk6Bk1Pg5BQ4OQVOToGTU+DkFDi5r/CCTwE5labXAAAAAElFTkSuQmCC\n"},"metadata":{},"execution_count":14}],"source":["# Imágen de cómo empieza el juego \n","env.reset()\n","PIL.Image.fromarray(env.render())"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"N7brXNIGWXjC","executionInfo":{"status":"ok","timestamp":1680681456599,"user_tz":-120,"elapsed":769,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["# Entornos de entrenamiento y validación de TF Agents\n","train_py_env = suite_atari.load(\n","    env_name,\n","    max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,\n","    gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n","\n","eval_py_env = suite_atari.load(\n","    env_name,\n","    max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,\n","    gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n","\n","train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"EoLEdYvzUHeX","executionInfo":{"status":"ok","timestamp":1680681464347,"user_tz":-120,"elapsed":10,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["# AtariPreprocessing corre 4 frames al mismo tiempo, max-pooling sobre los últimos 2 frames. \n","# Habrá que tenerlo en cuenta al actualizar intervalos\n","ATARI_FRAME_SKIP = 4\n","\n","class AtariCategoricalQNetwork(network.Network):\n","  \"\"\"CategoricalQNetwork subclass that divides observations by 255.\"\"\"\n","\n","  def __init__(self, input_tensor_spec, action_spec, **kwargs):\n","    super(AtariCategoricalQNetwork, self).__init__(\n","        input_tensor_spec, state_spec=())\n","    input_tensor_spec = tf.TensorSpec(\n","        dtype=tf.float32, shape=input_tensor_spec.shape)\n","    self._categorical_q_network = categorical_q_network.CategoricalQNetwork(\n","        input_tensor_spec, action_spec, **kwargs)\n","\n","  @property\n","  def num_atoms(self):\n","    return self._categorical_q_network.num_atoms\n","\n","  def call(self, observation, step_type=None, network_state=()):\n","    state = tf.cast(observation, tf.float32)\n","    # We divide the grayscale pixel values by 255 here rather than storing\n","    # normalized values beause uint8s are 4x cheaper to store than float32s.\n","    # TODO(b/129805821): handle the division by 255 for train_eval_atari.py in\n","    # a preprocessing layer instead.\n","    state = state / 255\n","    return self._categorical_q_network(\n","        state, step_type=step_type, network_state=network_state)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"TgkdEPg_muzV","executionInfo":{"status":"ok","timestamp":1680681467734,"user_tz":-120,"elapsed":1016,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["fc_layer_params = (512,)\n","conv_layer_params=((32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1))\n","\n","q_net = AtariCategoricalQNetwork(\n","            train_env.observation_spec(),\n","            train_env.action_spec(),\n","            conv_layer_params=conv_layer_params,\n","            fc_layer_params=fc_layer_params)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"jbY4yrjTEyc9","executionInfo":{"status":"ok","timestamp":1680681468029,"user_tz":-120,"elapsed":6,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["optimizer = tf.compat.v1.train.RMSPropOptimizer(\n","    learning_rate=learning_rate,\n","    decay=0.95,\n","    momentum=0.0,\n","    epsilon=0.00001,\n","    centered=True)\n","\n","train_step_counter = tf.Variable(0)\n","\n","observation_spec = tensor_spec.from_spec(train_env.observation_spec())\n","time_step_spec = ts.time_step_spec(observation_spec)\n","\n","action_spec = tensor_spec.from_spec(train_env.action_spec())\n","target_update_period=32000  # ALE frames\n","update_period=16  # ALE frames\n","_update_period = update_period / ATARI_FRAME_SKIP\n","\n","\n","agent = categorical_dqn_agent.CategoricalDqnAgent(\n","    time_step_spec,\n","    action_spec,\n","    categorical_q_network=q_net,\n","    optimizer=optimizer,\n","    #epsilon_greedy=epsilon,   # Probar \n","    n_step_update=1.0,\n","    target_update_tau=1.0,\n","    target_update_period=(\n","        target_update_period / ATARI_FRAME_SKIP / _update_period),\n","    gamma=0.99,\n","    reward_scale_factor=1.0,\n","    gradient_clipping=None,\n","    debug_summaries=False,\n","    summarize_grads_and_vars=False)\n","\n","# DQN Agent: Se pueden usar ambos, el Categorical me ha dado mejores resultados.\n","\"\"\"\n","agent = dqn_agent.DqnAgent(\n","    time_step_spec,\n","    action_spec,\n","    q_network=q_net,\n","    optimizer=optimizer,\n","    epsilon_greedy=0.01,\n","    n_step_update=1.0,\n","    target_update_tau=1.0,\n","    target_update_period=(\n","        target_update_period / ATARI_FRAME_SKIP / _update_period),\n","    td_errors_loss_fn=common.element_wise_huber_loss,\n","    gamma=0.99,\n","    reward_scale_factor=1.0,\n","    gradient_clipping=None,\n","    debug_summaries=False,\n","    summarize_grads_and_vars=False,\n","    train_step_counter=_global_step)\n","\"\"\"\n","\n","\n","agent.initialize()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"bitzHo5_UbXy","executionInfo":{"status":"ok","timestamp":1680681472131,"user_tz":-120,"elapsed":11,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["def compute_avg_return(environment, policy, num_episodes=10):\n","\n","  total_return = 0.0\n","  for _ in range(num_episodes):\n","\n","    time_step = environment.reset()\n","    episode_return = 0.0\n","\n","    while not time_step.is_last():\n","      action_step = policy.action(time_step)\n","      time_step = environment.step(action_step.action)\n","      episode_return += time_step.reward\n","    total_return += episode_return\n","\n","  avg_return = total_return / num_episodes\n","  return avg_return.numpy()[0]\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"vX2zGUWJGWAl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680681476150,"user_tz":-120,"elapsed":3723,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}},"outputId":"a4416bb1-def2-4be8-ca2f-a79cd8d673f4"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.counter(...)` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n"]}],"source":["replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","    data_spec=agent.collect_data_spec,\n","    batch_size=train_env.batch_size,\n","    max_length=replay_buffer_max_length)\n","\n","# Dataset genera trayectorias con shape [Bx2x...]\n","dataset = replay_buffer.as_dataset(\n","    num_parallel_calls=3, \n","    sample_batch_size=batch_size, \n","    num_steps=2).prefetch(3)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"wr1KSAEGG4h9","executionInfo":{"status":"ok","timestamp":1680681478071,"user_tz":-120,"elapsed":1929,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["# Política random para el video de comparación\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n","                                                train_env.action_spec())\n","\n","def collect_step(environment, policy, buffer):\n","  time_step = environment.current_time_step()\n","  action_step = policy.action(time_step)\n","  next_time_step = environment.step(action_step.action)\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","  # Add trajectory to the replay buffer\n","  buffer.add_batch(traj)\n","\n","def collect_data(env, policy, buffer, steps):\n","  for _ in range(steps):\n","    collect_step(env, policy, buffer)\n","\n","collect_data(train_env, random_policy, replay_buffer, \\\n","             steps=initial_collect_steps)"]},{"cell_type":"markdown","metadata":{"id":"-2XvqnF3H4Mj"},"source":["# Aquí está la magia (donde se entrena el modelo)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"0pTbJ3PeyF-u","colab":{"base_uri":"https://localhost:8080/","height":396},"executionInfo":{"status":"error","timestamp":1680681498459,"user_tz":-120,"elapsed":16630,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}},"outputId":"c40dcd8e-d296-44b7-8dac-af5d2d0ea7ed"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-39efa8fabcb5>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_traceback_filtering_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;31m# In some very rare cases,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_variable_creation_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m    \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         ._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    765\u001b[0m             *args, **kwds))\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_concrete_function_internal_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplaceholder_bound_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           concrete_function = self._create_concrete_function(\n\u001b[0m\u001b[1;32m    397\u001b[0m               args, kwargs, func_graph)\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     concrete_function = monomorphic_function.ConcreteFunction(\n\u001b[0;32m--> 300\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m       loss_info = self._train_fn(\n\u001b[0m\u001b[1;32m    331\u001b[0m           experience=experience, weights=weights, **kwargs)\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/agents/dqn/dqn_agent.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m       loss_info = self._loss(\n\u001b[0m\u001b[1;32m    394\u001b[0m           \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0mtd_errors_loss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_td_errors_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/agents/categorical_dqn/categorical_dqn_agent.py\u001b[0m in \u001b[0;36m_loss\u001b[0;34m(self, experience, td_errors_loss_fn, gamma, reward_scale_factor, weights, training)\u001b[0m\n\u001b[1;32m    443\u001b[0m                      'total_loss': total_loss}\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m       common.summarize_scalar_dict(dict_losses,\n\u001b[0m\u001b[1;32m    446\u001b[0m                                    \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                                    name_scope='Losses/')\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36msummarize_scalar_dict\u001b[0;34m(name_data, step, name_scope)\u001b[0m\n\u001b[1;32m   1402\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m           tf.compat.v2.summary.scalar(\n\u001b[0m\u001b[1;32m   1405\u001b[0m               name=name, data=data, step=step)\n\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorboard/plugins/scalar/summary_v2.py\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(name, data, step, description)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     summary_scope = (\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"summary_scope\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     )\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorboard.summary._tf.summary' has no attribute 'experimental'"]}],"source":["# Entrenamos\n","iterator = iter(dataset)\n","\n","# (Optional) Optimize by wrapping some of the code in a graph \n","# using TF function.\n","agent.train = common.function(agent.train)\n","\n","# Reset the train step\n","agent.train_step_counter.assign(0)\n","\n","# Evaluate the agent's policy once before training.\n","avg_return = compute_avg_return(eval_env, agent.policy, \\\n","                                num_eval_episodes)\n","returns = [avg_return]\n","\n","for _ in range(num_iterations):\n","\n","  # Collect a few steps using collect_policy and save to the replay buffer.\n","  for _ in range(collect_steps_per_iteration):\n","    collect_step(train_env, agent.collect_policy, replay_buffer)\n","\n","  # Sample a batch of data from the buffer and update the agent's network.\n","  experience, unused_info = next(iterator)\n","  train_loss = agent.train(experience).loss\n","\n","  step = agent.train_step_counter.numpy()\n","\n","  if step % log_interval == 0:\n","    print('step = {0}: loss = {1}'.format(step, train_loss))\n","\n","  if step % eval_interval == 0:\n","    avg_return = compute_avg_return(eval_env, agent.policy, \\\n","                                    num_eval_episodes)\n","    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n","    returns.append(avg_return)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxtL1mbOYCVO","executionInfo":{"status":"aborted","timestamp":1680681377827,"user_tz":-120,"elapsed":23,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["\n","iterations = range(0, num_iterations + 1, eval_interval)\n","plt.plot(returns)\n","plt.ylabel('Average Return')\n","plt.xlabel('Iterations')\n","plt.ylim(top=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULaGr8pvOKbl","executionInfo":{"status":"aborted","timestamp":1680681377828,"user_tz":-120,"elapsed":24,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["# Funciones para ver los videos en Colab y guardarlos.\n","def embed_mp4(filename):\n","  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n","  video = open(filename,'rb').read()\n","  b64 = base64.b64encode(video)\n","  tag = '''\n","  <video width=\"640\" height=\"480\" controls>\n","    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n","  Your browser does not support the video tag.\n","  </video>'''.format(b64.decode())\n","\n","  return IPython.display.HTML(tag)\n","\n","def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n","  filename = filename + \".mp4\"\n","  with imageio.get_writer(filename, fps=fps) as video:\n","    for _ in range(num_episodes):\n","      time_step = eval_env.reset()\n","      video.append_data(eval_py_env.render())\n","      while not time_step.is_last():\n","        action_step = policy.action(time_step)\n","        time_step = eval_env.step(action_step.action)\n","        video.append_data(eval_py_env.render())\n","  return embed_mp4(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owOVWB158NlF","executionInfo":{"status":"aborted","timestamp":1680681377828,"user_tz":-120,"elapsed":24,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["create_policy_eval_video(agent.policy, \"Modelo-entrenado\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJZIdC37yNH4","executionInfo":{"status":"aborted","timestamp":1680681377829,"user_tz":-120,"elapsed":25,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":["create_policy_eval_video(random_policy, \"Modelo-politica-random\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oj3-41pXldw1","executionInfo":{"status":"aborted","timestamp":1680681377830,"user_tz":-120,"elapsed":25,"user":{"displayName":"Valero Laparra","userId":"00355299981903664579"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}